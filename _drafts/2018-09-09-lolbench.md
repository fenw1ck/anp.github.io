---
layout: post
title:  "lolbench: informing Rust compiler development by continuously benchmarking 'in the wild' programs"
date:   2018-09-09
categories: rust
---

I've been working on what you could call a performance regression test suite for generated Rust binaries, made up of "in the wild" Rust benchmarks that are compiled with many different compiler versions and measured. With good enough data, we hope to ensure existing baseline performance is preserved, measure the impact of proposed changes, and to ensure that previous performance regressions are not repeated. It's called "[lolbench][github]" because I needed a placeholder name but now I've been working on it for too long without thinking of a better one so it's the name.

This is turning out to be a lengthy post. If you are interested in Rust performance and want to instead hear about some of this work in audiovisual format, I gave a talk [in May 2018 covering the project's challenges and status][cf-talk]. Caveat: the recording has unfortunately low quality.

If you don't have time to read this but the project sounds interesting to you, come help! Check out [this issue on GitHub][quest-issue] to see a list of benchmarks we know we'd like to add. If you have code that has been made slower by a previous release of Rust, send a PR! The README [has instructions for adding new benchmarks][adding-benches] if you'd like to contribute, and there are some other issues with opportunities to help at the [GitHub repo][github]. I've been posting about my progress in the `#wg-codegen` channel on irc.mozilla.org as `anp`, and am more than happy to chat there!

# Getting in Tune

*If either of the phrases "lolbench aims to be the new [Is Rust Fast Yet](http://huonw.github.io/isrustfastyet/)" or "lolbench aims to be a mashed up bors/perf.rlo/crater for runtime benchmarks" is meaningful to you, then congratulations you should skip this section! You still can even if you didn't parse that, but I wanted to write this bit down somewhere.*

Rust is a Fast Programming Language. Rust programs are therefore "fast," especially so if you write them with the correct observations to the arcane ley lines of birth and death known as "lifetimes," and also remember to pass cargo the `--release` flag. Without getting bogged down in what exactly "fast" means, it's a generally accepted cultural value in the Rust community that code provided to rustc should result in "fast" programs. And, as a matter of course, that if you upgrade your compiler in the sesquilunar ritual, the compiler should produce programs at least as "fast" as it did before.

Also a value in the Rust community, the [Not Rocket Science Rule of Software Engineering][not-rocket-science]:

> automatically maintain a repository of code that always passes all the tests

This is an approach to testing which has served the Rust project extremely well in many ways, and in my opinion execution on this approach has been a really significant factor in Rust's success to date. See [brson's post about testing the Rust project][brson-testing] for more details, although the post is a little old so some may be dated.

In the interest of preserving and advancing Rust as a Fast Programming Language, might we further propose that the project also adopt the "More Like Rocket Science Rule of Software Engineering"?

> automatically maintain a repository of code that always maintains or improves all the benchmarks

This is, for a litany of reasons, really really hard. Or, as I am told professionals say, quite challenging.

OK, maybe we could try for the "A Little Bit Like Rocket Science Rule of Software Engineering"?

> automatically maintain a repository of code where you are always notified if **benchmark results get worse**

This is an analog to the "you can't manage what you don't measure" aphorism and is implemented today for compile times in the form of [perf.rlo][prlo], which measures compile times for a number of projects using compilers built from every merge to master. There's an accompanying bot that can also be invoked on a PR before it is merged to see what effect the changes may have to downstream compile times.

Where perf.rlo measures the performance of rustc as it compiles "in the wild" code samples, lolbench measures the performance of "in the wild" code samples that have been compiled by rustc. See, the difference is so obvious!

I think this is an interesting project for a few reasons. For one, I got curious about it a couple of years ago (see below) and bounced off of it, so there's a redemption-slash-vendetta angle. Also, it's a little novel, at least to me. I'm not familiar with any other efforts to guide compiler development with continuously measured benchmarks collected primarily from community code. I also really like working on performance-sensitive code, and I really like Rust. Go figure. It gives me warm and fuzzies to build something (optimistically) useful to help Rust maintain an edge in this niche.

# Overture

In January 2016, Rust 1.8 was the current stable release, and Niko [posted a GitHub issue](https://github.com/rust-lang/rust/issues/31265), saying:

> We need a benchmark suite targeting the runtime of generated code.

Around the time, I wrote a [couple][6-months-runtime] ([reddit thread][6-months-reddit]) of [posts][getrusage] ([reddit thread][getrusage-reddit]) detailing my efforts to collect some benchmarks and identify clear performance changes. I ended up moving on to work on other projects after that, since the data wasn't very actionable. Niko then began collecting https://github.com/nikomatsakis/rust-runtime-benchmarks with the hope of it being the seed of something, which went dormant for a while. Then, about two years after his first post, he [posted to irlo][irlo-post] asking for help resuming work on the project.

I'm such a sucker.

# Won't Get Fooled Again



 Have I mentioned that measuring things on computers is like, really complex? Especially these days with all these new fangled things like "the memory hierarchy," "," and of course "multiprocessing."

  * measuring heterogeneous microbenchmarks in a reliable way is *really* difficult and I did not know enough about computers to sufficiently control for sources of noise in the data I collected
  * if you think of each benchmark as "covering" some portion of the compiler's possible output, to my knowledge we don't yet have a model for describing "coverage" for the possible binaries that a compiler could emit or the portions of the compiler's understanding of a language exercised by a given compilation, so you need many different benchmarks to hopefully spray-and-pray your way to covering a large language like Rust
  * if you need lots (order 100 or 1000) of benchmarks to usefully describe Rust code that a compiler should optimize well, you can't rely on humans to review all of the results for every change to the compiler
  * I did not know enough about statistics to define a useful model for automatically flagging regressions based on the data I collected
  * there can be a *lot* of manual work involved in adapting benchmarks I didn't write to a different runner/framework
  * it takes a long time to run a lot of benchmarks, keeping up with nightlies is reasonable but keeping up with every merge is very hard without coordinating multiple benchmark runners, much less running hypothetical PRs to see how they do before merge
  * if you want to parallelize the work across multiple machines, you probably want to "pin" a benchmark to a particular machine so that you are making apples-to-apples comparisons
  * benchmark setup cost is hard to measure with perf cli
  * `cargo bench` would have made it difficult to:

[github]: https://github.com/anp/lolbench
[github-data]: https://github.com/anp/lolbench-data
[adding-benches]: https://github.com/anp/lolbench/TODO
[quest-issue]: https://github.com/anp/lolbench/TODO
[6-months-runtime]: /rust/2016/02/16/almost-6-months-rust-runtime-perf/
[6-months-reddit]: https://www.reddit.com/r/rust/comments/4616ny/almost_6_months_of_rust_runtime_performance/
[getrusage]: /rust/2016/02/24/rust-runtime-cargobench-vs-getrusage/
[getrusage-reddit]: https://www.reddit.com/r/rust/comments/47dohh/measuring_rust_runtime_performance_cargo_bench_vs/
[cf-talk]: https://www.youtube.com/watch?v=gSFTbJKScU0
[prlo]: https://perf.rust-lang.org
[irlo-post]: https://internals.rust-lang.org/t/help-needed-corpus-for-measuring-runtime-performance-of-generated-code/6794
[not-rocket-science]: https://graydon2.dreamwidth.org/1597.html
[bors]: TODO
[brson-testing]: TODO
